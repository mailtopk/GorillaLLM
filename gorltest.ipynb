{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gorilla LLM \n",
    "It is trained on earning hub datasets: Torch Hub, TensorFlow Hub and HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Vision API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "openai.api_key = \"EMPTY\" # Key is ignored and does not matter. \n",
    "openai.api_base = \"http://zanino.millennium.berkeley.edu:8000/v1\" # Open endpoint hosted by berkeley\n",
    "\n",
    "def get_gorilla_response(prompt, model):\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model = model,\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return completion.choices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diff models supported \n",
    "\n",
    "gorilla-cli-v0\n",
    "gorilla-azure-cli-v0\n",
    "gorilla-7b-th-v0\n",
    "gorilla-aws-cli-v0\n",
    "gorilla-mpt-7b-hf-v0\n",
    "gorilla-7b-hf-v1\n",
    "gorilla-gcp-cli-v0\n",
    "gorilla-7b-hf-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"<<<domain>>>: Multimodal Image-to-Text\\n<<<api_call>>>: model = Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. from transformers import Pix2StructForConditionalGeneration, Pix2StructTokenizer imports the necessary components from the Hugging Face Transformers library.\\n2. model and tokenizer are initialized with the predefined model checkpoint.\\n3. image_path is the path to the image you want to process.\\n4. An instance of the image is opened using Image.open(image_path).\\n5. The model's predictions are generated by passing the image and prompt to the tokenizer, which encodes the image into a format suitable for the model.\\n6. The model's output is generated by passing the encoded image and prompt to the model.\\n7. The predicted text is decoded from the model's output.<<<code>>>:\\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructTokenizer\\nfrom PIL import Image\\n\\ndef load_model():\\n    model_checkpoint = 'google/pix2struct-textcaps-base'\\n    tokenizer = Pix2StructTokenizer.from_pretrained(model_checkpoint)\\n    model = Pix2StructForConditionalGeneration.from_pretrained(model_checkpoint)\\n    return tokenizer, model\\n\\ndef process_data(image_path, tokenizer, model):\\n    image = Image.open(image_path)\\n    prompt = \\\"an image of a bird\\\"\\n    inputs = tokenizer(prompt, images=image, return_tensors='pt')\\n    outputs = model.generate(**inputs)\\n    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\\n    return response[0]\\n\\nimage_path = 'bird.png'\\n\\n# Load the model and tokenizer\\ntokenizer, model = load_model()\\n\\n# Process the data\\nresponse = process_data(image_path, tokenizer, model)\\n\\nprint(\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = \"gorilla-mpt-7b-hf-v0\"\n",
    "model = 'gorilla-7b-hf-v1'\n",
    "\n",
    "prompt = \"Given an image of the bird, I want to know the details of it\"\n",
    "result = get_gorilla_response(prompt=prompt, model=model)\n",
    "print(json.dumps(result.message, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<domain>>>: Multimodal Image-to-Text\n",
      "<<<api_call>>>: model = Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')\n",
      "<<<api_provider>>>: Hugging Face Transformers\n",
      "<<<explanation>>>: 1. from transformers import Pix2StructForConditionalGeneration, Pix2StructTokenizer imports the necessary components from the Hugging Face Transformers library.\n",
      "2. model and tokenizer are initialized with the predefined model checkpoint.\n",
      "3. image_path is the path to the image you want to process.\n",
      "4. An instance of the image is opened using Image.open(image_path).\n",
      "5. The model's predictions are generated by passing the image and prompt to the tokenizer, which encodes the image into a format suitable for the model.\n",
      "6. The model's output is generated by passing the encoded image and prompt to the model.\n",
      "7. The predicted text is decoded from the model's output.<<<code>>>:\n",
      "from transformers import Pix2StructForConditionalGeneration, Pix2StructTokenizer\n",
      "from PIL import Image\n",
      "\n",
      "def load_model():\n",
      "    model_checkpoint = 'google/pix2struct-textcaps-base'\n",
      "    tokenizer = Pix2StructTokenizer.from_pretrained(model_checkpoint)\n",
      "    model = Pix2StructForConditionalGeneration.from_pretrained(model_checkpoint)\n",
      "    return tokenizer, model\n",
      "\n",
      "def process_data(image_path, tokenizer, model):\n",
      "    image = Image.open(image_path)\n",
      "    prompt = \"an image of a bird\"\n",
      "    inputs = tokenizer(prompt, images=image, return_tensors='pt')\n",
      "    outputs = model.generate(**inputs)\n",
      "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      "    return response[0]\n",
      "\n",
      "image_path = 'bird.png'\n",
      "\n",
      "# Load the model and tokenizer\n",
      "tokenizer, model = load_model()\n",
      "\n",
      "# Process the data\n",
      "response = process_data(image_path, tokenizer, model)\n",
      "\n",
      "print(\n"
     ]
    }
   ],
   "source": [
    "print(result.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try kubernetes API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"<<<domain>>>: Kubernetes\\n<<<api_call>>>: image_ids = kubectl get pods -n namesapce -o=jsonpath='{.images}'\\n<<<api_provider>>>: Kubernetes\\n<<<explanation>>>: 1. Load the kubectl and jsonpath libraries.\\n2. Get the list of pods in the specified namespace using the kubectl get command.\\n3. Use the jsonpath function to extract the image ids from the pods list.<<<code>>>:\\n\\nimport json\\nimport kubectl\\n\\ndef load_model():\\n    # return the kubectl command to get the image ids of pods\\n    return kubectl.NewCommand(\\n        'get',\\n        'pods',\\n        '-n',\\n        'namesapce',\\n        '-o',\\n        'jsonpath={.images}'\\n    )\\n\\ndef process_data(image_ids, next_token):\\n    # return a list of dictionaries with pod info and image id\\n    response = []\\n    for image_id in image_ids:\\n        pod_info = {\\n            'image_id': image_id,\\n            'next_token': next_token\\n        }\\n        response.append(pod_info)\\n    return response\\n\\n# Load the model\\nimage_ids = load_model()\\n\\n# Process the data\\nresponse = process_data(image_ids, 'next_token')\\n\\n# Print the results\\nprint(response)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = 'gorilla-7b-hf-v1'\n",
    "#model = 'gorilla-gcp-cli-v0' #this model is returning google cloud API\n",
    "prompt = \"give me a kubectl command to get the image ids of all pods running a namespaces in kubernetes\"\n",
    "result = get_gorilla_response(prompt=prompt, model=model)\n",
    "print(json.dumps(result.message, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<domain>>>: Kubernetes\n",
      "<<<api_call>>>: image_ids = kubectl get pods -n namesapce -o=jsonpath='{.images}'\n",
      "<<<api_provider>>>: Kubernetes\n",
      "<<<explanation>>>: 1. Load the kubectl and jsonpath libraries.\n",
      "2. Get the list of pods in the specified namespace using the kubectl get command.\n",
      "3. Use the jsonpath function to extract the image ids from the pods list.<<<code>>>:\n",
      "\n",
      "import json\n",
      "import kubectl\n",
      "\n",
      "def load_model():\n",
      "    # return the kubectl command to get the image ids of pods\n",
      "    return kubectl.NewCommand(\n",
      "        'get',\n",
      "        'pods',\n",
      "        '-n',\n",
      "        'namesapce',\n",
      "        '-o',\n",
      "        'jsonpath={.images}'\n",
      "    )\n",
      "\n",
      "def process_data(image_ids, next_token):\n",
      "    # return a list of dictionaries with pod info and image id\n",
      "    response = []\n",
      "    for image_id in image_ids:\n",
      "        pod_info = {\n",
      "            'image_id': image_id,\n",
      "            'next_token': next_token\n",
      "        }\n",
      "        response.append(pod_info)\n",
      "    return response\n",
      "\n",
      "# Load the model\n",
      "image_ids = load_model()\n",
      "\n",
      "# Process the data\n",
      "response = process_data(image_ids, 'next_token')\n",
      "\n",
      "# Print the results\n",
      "print(response)\n"
     ]
    }
   ],
   "source": [
    "print(result.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gorlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
